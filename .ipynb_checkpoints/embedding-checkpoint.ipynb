{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "from pandas import DataFrame, Series\n",
    "from keras import models, layers, optimizers, losses, metrics\n",
    "from keras.utils.np_utils import to_categorical\n",
    " \n",
    "#深度学习用于自然语言处理是将模式识别应用于单词、 句子和段落\n",
    "#文本向量化（vectorize）是指将文本转换为数值张量的过程\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#单词级的one-hot编码，之前路透社及imdb数据集已经涉及\n",
    "#它将每个单词与一个唯一的整数索引相关联， 然后将这个整数索引 i 转换为长度为N的二进制向量（N是词表大小），这个向量只有第 i 个元 素是 1，其余元素都为 0。\n",
    "#one-hot 编码得到的向量是二进制的、稀疏的（绝大部分元素都是0）、维度很高的（维度大小等于词表中的单词个数）\n",
    "def one_hot_word():\n",
    "    samples=['The cat sat on the mat.','The dog ate my homework.']\n",
    "    token_index={}\n",
    "    for sample in samples:\n",
    "        for word in sample.split():#按空格划分单词（这里并没有去除标点符号）\n",
    "            if word not in token_index:#为每个单词建立索引，从1开始（单词不重复）\n",
    "                token_index[word]=len(token_index)+1\n",
    "    max_length=10#samples中的不重复分词长度为10，且每个samples元素分词长度最大为6\n",
    "    results=np.zeros(shape=(len(samples),max_length,max(token_index.values())+1))\n",
    "    print(token_index)\n",
    "    print(results.shape)\n",
    "    print(results)\n",
    "    for i,sample in enumerate(samples):\n",
    "        for j,word in list(enumerate(sample.split()))[:max_length]:\n",
    "            index=token_index.get(word)\n",
    "            results[i,j,index]=1.#索引与数组下标相同置为1\n",
    "    return results\n",
    "# one_hot_word()\n",
    " \n",
    "#字符级one-hot编码\n",
    "import string\n",
    "def one_hot_char():\n",
    "    samples=['The cat sat on the mat.','The dog ate my homework.']\n",
    "    characters=string.printable#返回字符串，所有可打印的 ASCII 字符\n",
    "    print(characters)\n",
    "    token_index=dict(zip(range(1,len(characters)+1),characters))#给字符添加索引\n",
    "    print(token_index)\n",
    "    max_length=50\n",
    "    results=np.zeros((len(samples),max_length,max(token_index.keys())+1))\n",
    "    for i,sample in enumerate(samples):\n",
    "        for j,character in enumerate(sample):\n",
    "            index=token_index.get(character)\n",
    "            results[i,j,index]=1.\n",
    "    return results\n",
    "# one_hot_char()\n",
    "#------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Keras 的内置函数可以对原始文本数据进行单词级或字符级的 one-hot 编码\n",
    "有很多重要的特性：\n",
    "从字符串中去除特殊字符、只考虑数据集中前N个最常见的单词（这是一种常用的限制，以避免处理非常大的输入向量空间）\n",
    "'''\n",
    "#用 Keras 实现单词级的 one-hot 编码\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "samples=['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "tokenizer=Tokenizer(num_words=1000)#创建一个分词器（tokenizer），设置 为只考虑前 1000 个最常见的单词\n",
    "tokenizer.fit_on_texts(samples)#构建单词索引\n",
    "sequences = tokenizer.texts_to_sequences(samples)#将字符串转换为整数索引组成的列表\n",
    "print(sequences)\n",
    "one_hot_results=tokenizer.texts_to_matrix(samples,mode='binary')\n",
    "#可以直接得到 one-hot 二进制表示。 这个分词器也支持除 one-hot 编码外 的其他向量化模式\n",
    "word_index=tokenizer.word_index#返回单词索引\n",
    "print(word_index)\n",
    "print('found %s unique tokens.'%len(word_index))\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用散列技巧的单词级的 one-hot 编码[hash]\n",
    "#如果词表中唯一标记的数量太大而无法直接处理，就可以使用这种技巧\n",
    "#将单词散列编码为固定长度的向量，通常用一个非常简单的散列函数来实现,但可能会出现散列冲突[如果散列空间的维度远大于需要散列的唯一标记的个数，散列冲突的可能性会减小]\n",
    " \n",
    "#使用散列技巧的单词级的 one-hot 编码\n",
    "samples=['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "dimensionality=1000#将单词保存为长度为 1000 的向量。如果单词数量接近 1000 个（或更多），那么会遇到很多散列冲突，这会降低这种编码方法的准确性\n",
    "max_length=10\n",
    "results=np.zeros((len(samples), max_length, dimensionality))\n",
    "print(results.shape)\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        index = abs(hash(word)) % dimensionality #将单词散列为 0~1000 范围内的一个随机整数索引\n",
    "        results[i, j, index] = 1.\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot 编码或 one-hot 散列得到的词表示是稀疏的、高维的、硬编码的；\n",
    "# 词嵌入是密集的、相对低维的，而且是从数据中学习得到的\n",
    "#使用词嵌入（word embedding）：\n",
    "'''\n",
    "获取词嵌入有两种方法\n",
    "    1.在完成主任务（比如文档分类或情感预测）的同时学习词嵌入。在这种情况下，\n",
    "一开始是随机的词向量，然后对这些词向量进行学习，其学习方式与学习神经网络的\n",
    "权重相同\n",
    "    2.在不同于待解决问题的机器学习任务上预计算好词嵌入，然后将其加载到模型\n",
    "中。这些词嵌入叫作预训练词嵌入（pretrained word embedding）\n",
    "    \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.利用Embedding层学习嵌入\n",
    "'''\n",
    "    在一个合理的嵌入空间中，同义词应该被嵌入 到相似的词向量中，一般来说，任意两个词向量之间的几何距离（比如 L2 距离）应该和这两个词的语义距离有\n",
    "关（表示不同事物的词被嵌入到相隔很远的点，而相关的词则更加靠近）。除了 距离，你可能还希望嵌入空间中的特定方向也是有意义的\n",
    "    在真实的词嵌入空间中，常见的有意义的几何变换的例子包括“性别”向量和“复数”向量。例如，将 king（国王）向量加上 female（女性）向量，得到的是\n",
    "queen（女王）向量。将 king（国王） 向量加上 plural（复数）向量，得到的是 kings 向量。词嵌入空间通常具有几千个这种可解释的、 并且可能很有用的向量。\n",
    "    一个好的词嵌入空间在很大程度上取决于你的任务。英语电影评论情感分析 模型的完美词嵌入空间，可能不同于英语法律文档分类模型的完美词嵌入空间，因为某些语义关系的重要性因任务而异\n",
    "    合理的做法是对每个新任务都学习一个新的嵌入空间。幸运的是，反向传播让这种学习变得很简单，而 Keras 使其变得更简单\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将一个 Embedding 层实例化\n",
    "from keras.layers import Embedding\n",
    "embedding_layer=Embedding(1000,64)#Embedding 层至少需要两个参数： 标记的个数（即最大单词索引 +1）和嵌入的维度（这里是 64）\n",
    "'''\n",
    "接收整数作为输入，并在内部字典中查找这些整数，然后返回相关联的向量。\n",
    "     单词索引---> Embedding ----->对应的词向量\n",
    "输入: 一个二维整数张量，其形状为 (samples, sequence_length)，每个元素是一个整数序列[一批数据中的所有序列必须具有相同的长度（因为需要将它们打包成一个张量），所以较短的序列应该用 0 填充，较长的序列应该被截断。]\n",
    "输出：返回一个形状为 (samples, sequence_length, embedding_ dimensionality) 的三维浮点数张量\n",
    "'''\n",
    "#加载IMDB数据，准备用于Embedding层\n",
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n",
    "def general_embedding():\n",
    "    max_features=10000#作为特征的单词个数（前10000个常见单词）\n",
    "    maxlen=20#每个样本达到20个单词后截断文本\n",
    "    (x_train,y_train),(x_test,y_test)=imdb.load_data(num_words=max_features)\n",
    "    x_train=preprocessing.sequence.pad_sequences(x_train,maxlen=maxlen)#将整数列表转换成形状为(samples, maxlen)的二维整数张量\n",
    "    x_test=preprocessing.sequence.pad_sequences(x_test,maxlen=maxlen)\n",
    " \n",
    "    #在IMDB数据上使用Embedding层和分类器\n",
    "    from keras.layers import Flatten,Dense,Embedding\n",
    "    model=models.Sequential()\n",
    "    model.add(Embedding(10000,8,input_length=maxlen))#指定 Embedding 层的最大输入长度，以便后面将嵌入输入展平。Embedding层激活的形状为(samples, maxlen, 8)\n",
    "    model.add(Flatten())#将三维的嵌入张量展平成形状为 (samples, maxlen * 8) 的二维张量\n",
    "    model.add(Dense(1,activation='sigmoid'))#分类器\n",
    "    model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n",
    "    print(model.summary())\n",
    "    history=model.fit(x_train,y_train,epochs=10,batch_size=32,validation_split=0.2)\n",
    "# general_embedding()\n",
    "#val_acc: 0.7466\n",
    "'''\n",
    "注意，仅仅将嵌入序列展开并在上面训练一个 Dense 层，会导致模型对输入序列中的每个单词单独处理，而没有考虑单词之间的关系和句子结构（举个例子，这个\n",
    "模型可能会将 this movie is a bomb和 this movie is the bomb两条都归为负面评论）。更好的做法是在嵌入序列上添加循环层或一维卷积层，将每个序列作为整体来学习特征\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########-------------------------------------------------------------------\n",
    "#2.使用预训练的词嵌入\n",
    "#原理类似于之前的预训练的cnn（如VGG16等）：没有足够的数据来自己学习真正强大的特征，但你需要的特征应该是非常通用的， 比如常见的视觉特征或语义特征\n",
    "#1）无监督的方法计算一个密集的低维词嵌入空间：word2vec 算法 由 Google 的 Tomas Mikolov 于 2013 年开发，其维度抓住了特定的语义属性，比如性别；\n",
    "# 2）GloVe（global vectors for word representation，词表示全局向量），由斯坦福大学的研究人员于 2014 年开发。这种嵌入方法基于对词共现统计矩阵进行因式分解\n",
    " \n",
    "#####-------------------整合在一起：从原始文本到词嵌入\n",
    "#1.下载IMDB数据的原始文本：http://mng.bz/0tIo\n",
    "#[因为该文件不带后缀，可能导致浏览器无法访问，可尝试使用xunlei 输入网址下载，或者使用linux 命令行：wget http://mng.bz/0tIo --no-check-certificate ]\n",
    "#2.处理IMDB原始数据的标签\n",
    "import os\n",
    "imdb_dir='F:/aclImdb'#解压后文件路径\n",
    "train_dir=os.path.join(imdb_dir,'train')\n",
    "def word_splitting(directory):\n",
    " \n",
    "    labels=[]#存放标签\n",
    "    texts=[]#存放评论\n",
    "    for label_type in ['neg','pos']:\n",
    "        dir_name=os.path.join(directory,label_type)\n",
    "        for fname in os.listdir(dir_name):#neg/pos目录下所有.txt文件\n",
    "            if fname[-4:]=='.txt':\n",
    "                #出现UnicodeDecodeError: 'gbk' codec can't decode byte 0x89 in position 14: illegal multibyte sequence错误：添加encoding='utf-8'\n",
    "                f=open(os.path.join(dir_name,fname),encoding='utf-8')\n",
    "                texts.append(f.read())\n",
    "                f.close()\n",
    "                if label_type=='neg':#标记\n",
    "                    labels.append(0)\n",
    "                else:\n",
    "                    labels.append(1)\n",
    "    return texts,labels\n",
    "#2.对数据进行分词\n",
    "#针对于具体任务的嵌入可能效果更好\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "maxlen=100#100个单词后截断评论\n",
    "training_samples=200#因为预训练的词嵌入对训练数据很少的问题特别有用(将训练数据限定为前 200)\n",
    "validation_samples=10000#验证集\n",
    "max_words=10000#只考虑数据集中前 10 000 个最常见的单词\n",
    "texts,labels=word_splitting(train_dir)\n",
    "tokenizer=Tokenizer(num_words=max_words)#分词\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences=tokenizer.texts_to_sequences(texts)#转换成索引序列\n",
    "print(sequences)#\n",
    "word_index=tokenizer.word_index#获取词索引\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "data=pad_sequences(sequences,maxlen=maxlen)#将整数列表转换成形状为(samples, maxlen)的二维整数张量\n",
    "labels=np.asarray(labels)#标签列表转换成一维张量\n",
    "print(data.shape)\n",
    "print(labels.shape)\n",
    "indices=np.arange(data.shape[0])#texts（评论）数\n",
    "np.random.shuffle(indices)#置乱（不然所有负面评论都在前面， 然后是所有正面评论）\n",
    "data=data[indices]\n",
    "labels=labels[indices]\n",
    "x_train=data[:training_samples]\n",
    "y_train=labels[:training_samples]\n",
    "x_val=data[training_samples:training_samples+validation_samples]#验证集10000\n",
    "y_val=labels[training_samples:training_samples+validation_samples]\n",
    "print(x_val.shape)\n",
    " \n",
    "#3.下载GloVe词嵌入：https://nlp.stanford.edu/projects/glove  找到glove.6B.zip，解压[400 000 个单词（或非单词的标记） 的 100 维嵌入向量]\n",
    " \n",
    "#4.对嵌入进行预处理：构建一个将单词（字符串）映射为其向量表示（数值向量）的索引\n",
    "glove_dir='F:/glove.6B'\n",
    "embeddings_index={}\n",
    "f=open(os.path.join(glove_dir,'glove.6B.100d.txt'),encoding='utf-8')\n",
    "i=0\n",
    "for line in f:\n",
    "    values=line.split()#形式为['word','float','float',...,'float']#(即该单词与其它单词的相关因子)\n",
    "    word=values[0]\n",
    "    coefs=np.asarray(values[1:],dtype='float32')\n",
    "    embeddings_index[word]=coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    " \n",
    "#准备GloVe词嵌入矩阵\n",
    " \n",
    "#它必须是一个形状为 (max_words, embedding_dim) 的矩阵，对于单词索引（在分词时构建）中索引为 i 的单词， 这个矩阵的元素 i 就是这个单词对应的 embedding_dim 维向量\n",
    " \n",
    "embedding_dim=100\n",
    "embedding_matrix=np.zeros((max_words,embedding_dim))\n",
    "for word,i in word_index.items():\n",
    "    if i <max_words:\n",
    "        embedding_vector=embeddings_index.get(word)#获取嵌入向量\n",
    "        if embedding_vector is not None:#嵌入索引（embeddings_index） 中找不到的词，其嵌入向量全为 0\n",
    "            embedding_matrix[i]=embedding_vector\n",
    " \n",
    "#5.定义模型\n",
    "from keras.layers import Flatten,Dense,Embedding\n",
    "model=models.Sequential()\n",
    "model.add(Embedding(max_words,embedding_dim,input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "print(model.summary())\n",
    " \n",
    "#在模型中加载GloVe嵌入\n",
    "model.layers[0].set_weights([embedding_matrix])#Embedding 层只有一个权重矩阵，是一个二维的浮点数矩阵，其中每个元素 i 是与索引 i 相关联的词向量\n",
    "model.layers[0].trainable=False\n",
    "#要冻结Embedding层（即将其trainable属性设为False），如果一个模型的一部分是经过预训练的（如Embedding层），而另一部分是随机初始化的（如分类器），\n",
    "# 那么在训练期间不应该更新预训练的部分，以 避免丢失它们所保存的信息。随机初始化的层会引起较大的梯度更新，会破坏已经学到的特征\n",
    " \n",
    "#7.训练与评估\n",
    "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n",
    "history=model.fit(x_train,y_train,epochs=10,batch_size=32,validation_data=(x_val,y_val))\n",
    "model.save_weights('pre_trained_glove_model.h5')\n",
    " \n",
    "#8.绘制结果\n",
    "def acc_loss_plot(history):\n",
    "    fig=plt.figure()\n",
    "    ax1=fig.add_subplot(2,1,1)\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "    ax1.plot(epochs, acc, 'bo', label='Training acc')\n",
    "    ax1.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "    ax1.set_title('Training and validation accuracy')\n",
    "    ax2=fig.add_subplot(2,1,2)\n",
    "    ax2.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    ax2.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    ax2.set_title('Training and validation loss')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "acc_loss_plot(history)#训练样本很少，模型很快过拟合，验证精度接近60%\n",
    " \n",
    "#在不加载预训练词嵌入、也不冻结嵌入层的情况下训练相同的模型，精度达到50%左右------注释掉239，240行代码\n",
    " \n",
    "#对测试集进行分词\n",
    "test_dir=os.path.join(imdb_dir,'test')\n",
    "texts,labels=word_splitting(test_dir)\n",
    "sequences = tokenizer.texts_to_sequences(texts)#测试集不训练\n",
    "x_test=pad_sequences(sequences,maxlen)\n",
    "y_test=np.asarray(labels)\n",
    " \n",
    "#在测试集上评估模型\n",
    "model.load_weights('pre_trained_glove_model.h5')\n",
    "results=model.evaluate(x_test,y_test)\n",
    "print(results)#0.55596"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
